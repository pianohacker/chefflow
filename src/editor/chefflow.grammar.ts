// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {multiWordName} from "./tokens"
export const parser = LRParser.deserialize({
  version: 14,
  states: "$bQVQROOOOQO'#C_'#C_O[QQO'#C^OOQP'#Ce'#CeQVQROOOdQRO'#C`OiQQO,58xOqQQO,58xOOQP-E6c-E6cOOQP,58z,58zOOQP'#Cb'#CbOvQRO'#CaOOQP'#Ca'#CaO!OQRO1G.dOiQQO1G.dOOQP'#Cc'#CcOOQP'#Cd'#CdOOQP,58{,58{O!ZQRO,58{OiQQO'#CfO!`QRO7+$OO!`QRO7+$OOOQP1G.g1G.gOOQP,59Q,59QOOQP-E6d-E6dO!kQRO<<Gj",
  stateData: "!v~O_OS~O[PO~O`TOaUO~O[XO~O`TObYO~Oa^O~O[`O]_O~OccOZQi[Qi~O[`O~OccOZQq[Qq~OccOZQy[Qy~O",
  goto: "!eZPP[`dluz}!T!ZTROSTQOSQVQV[U^cQ]UQe^RgcVZU^cRbZQaZRfbQSORWSQd]ShdiRie",
  nodeNames: "⚠ Recipe Step StepDesc ResultName Ingredient Amount Unit IngredientType",
  maxTerm: 19,
  skippedNodes: [0],
  repeatNodeCount: 2,
  tokenData: "#r~R_X^!Qpq!Q|}!u!O!P!z!Q![#Y![!]#h!b!c#m#y#z!Q$f$g!Q#BY#BZ!Q$IS$I_!Q$I|$JO!Q$JT$JU!Q$KV$KW!Q&FU&FV!Q~!VY_~X^!Qpq!Q#y#z!Q$f$g!Q#BY#BZ!Q$IS$I_!Q$I|$JO!Q$JT$JU!Q$KV$KW!Q&FU&FV!Q~!zOc~~!}P!Q![#Q~#VPb~!Q![#Q~#_Rb~!O!P!z!P!Q!z!Q![#Y~#mOa~~#rO`~",
  tokenizers: [multiWordName, 0],
  topRules: {"Recipe":[0,1]},
  tokenPrec: 0,
  termNames: {"0":"⚠","1":"@top","2":"Step","3":"StepDesc","4":"ResultName","5":"Ingredient","6":"Amount","7":"Unit","8":"IngredientType","9":"Step+","10":"(\",\" Ingredient)+","11":"␄","12":"multiWordName","13":"unit","14":"%mainskip","15":"space","16":"\"@\"","17":"\":\"","18":"amount","19":"\",\""}
})
